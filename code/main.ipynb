{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debiassing Classification Variational Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from setup import Config\n",
    "from main import make_trainer, make_evaluator\n",
    "from logger import logger\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "# Change icons to be notebook compatible\n",
    "logger.set_notebook_syntax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising config & trainer\n",
    "Create a config file in which we can pass allong the required settings for the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-18 00:00:29,544 - DebiasingModel - INFO -  ðŸ’¿ Saving new run files to train \n",
      "\n",
      "2024-11-18 00:00:29,588 - DebiasingModel - INFO - Creating new model with the following parameters:\n",
      "z_dim: 200\n",
      "hist_size: 1000\n",
      "alpha: 0.01\n",
      "num_bins: 10\n",
      " \n",
      "\n",
      "2024-11-18 00:00:29,628 - DebiasingModel - INFO - Creating the celeb and imagenet dataset from the h5 file! \n",
      "\n",
      "2024-11-18 00:00:44,179 - DebiasingModel - INFO - Sizes of dataset are:\n",
      "Celeb-train: 43965\n",
      "Celeb-valid: 10992\n",
      "Imagenet-train: 43965\n",
      "Imagenet-valid: 10992\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a trainer, train\n",
    "config = Config(run_folder='train', debias_type='chow-liu')\n",
    "trainer = make_trainer(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the architecture of the encoder and decoder\n",
    "First we have a look on the architecture of the encodering and decoding networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Db_vae(\n",
       "  (encoder): Encoder(\n",
       "    (layers): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(2, 2))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2))\n",
       "      (4): LeakyReLU(negative_slope=0.01)\n",
       "      (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2))\n",
       "      (7): LeakyReLU(negative_slope=0.01)\n",
       "      (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): Conv2d(256, 512, kernel_size=(5, 5), stride=(2, 2))\n",
       "      (10): LeakyReLU(negative_slope=0.01)\n",
       "      (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): Flatten()\n",
       "      (13): Linear(in_features=512, out_features=1000, bias=True)\n",
       "      (14): LeakyReLU(negative_slope=0.01)\n",
       "      (15): Linear(in_features=1000, out_features=401, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=200, out_features=1000, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1000, out_features=512, bias=True)\n",
       "      (3): UnFlatten()\n",
       "      (4): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2))\n",
       "      (5): LeakyReLU(negative_slope=0.01)\n",
       "      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (7): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(2, 2))\n",
       "      (8): LeakyReLU(negative_slope=0.01)\n",
       "      (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2), output_padding=(1, 1))\n",
       "      (11): LeakyReLU(negative_slope=0.01)\n",
       "      (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (13): ConvTranspose2d(64, 3, kernel_size=(5, 5), stride=(2, 2), output_padding=(1, 1))\n",
       "      (14): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the training for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the recreated images by the VAE in a 10 by 10 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing the recreation of actual images by the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recreate actual images by the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.reconstruction_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing which images are best predicted and worst predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can analyse the best and worst predicted images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.best_and_worst()\n",
    "#TODO: Fix this, need to convert from Long to float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an evaluation object\n",
    "We load in the model from the folder **train**. Set `path_to_model` to any folder in `results/` with a model.pt to load in that corresponding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config = Config(path_to_model='train')\n",
    "evaluator = make_evaluator(eval_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the actual evaluation on the defined subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model performance, and save the results in the same area where the model was extracted from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.eval_on_setups(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our own final results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Included in this notebook is an analysis of our own results for max5, based on the training of 5 different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_results = pd.read_csv(\"paper_results/results.csv\")positive semidefinite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Recall of the non-debiased and max-based debiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall setup\n",
    "colors = [\"#fffecb\", \"#a1dab4\", \"#41b6c4\", \"#2c7fb8\", \"#253594\"]\n",
    "training_type = [\"no debias\", r\"$\\alpha$ = 0.1\", r\"$\\alpha$ = 0.05\", r\"$\\alpha$ = 0.01\", r\"$\\alpha$ = 0.001\"]\n",
    "setups = [\"dark male\", \"dark female\", \"light male\", \"light female\", \"recall\"]\n",
    "name_list = [\"train1_\", \"train2_\", \"train3\", \"train4\", \"train5\"]\n",
    "\n",
    "utils.make_bar_plot(df_final_results, name_list, setups, training_type=training_type, colors=colors, y_lim=(80, 105), y_label=\"Recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall and Accuracy of the non-debiased and max-based debiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to extract precision recall and accuracy\n",
    "colors = [\"#fffecb\", \"#a1dab4\", \"#41b6c4\", \"#2c7fb8\", \"#253594\"]\n",
    "training_type = [\"no debias\", r\"$\\alpha$ = 0.1\", r\"$\\alpha$ = 0.05\", r\"$\\alpha$ = 0.01\", r\"$\\alpha$ = 0.001\"]\n",
    "name_list = [\"train1_\", \"train2\", \"train3\", \"train4\", \"train5\"]\n",
    "setups = [\"precision\", \"recall\", \"accuracy\"]\n",
    "\n",
    "utils.make_bar_plot(df_final_results, name_list, setups, training_type=training_type, colors=colors, y_lim=(60,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance of non-debiased, max and gaussian models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the colors\n",
    "col1 = '#FFEC00'\n",
    "col2 = '#FF5300'\n",
    "col3 = '#00AB6F'\n",
    "col4 = '#580EAD'\n",
    "colors = [col1, col2, col2, col2, col2,\n",
    "                col3, col3, col3, col3,\n",
    "                col4, col4, col4, col4]\n",
    "\n",
    "# Define the setup names\n",
    "name_list = [f\"train{i}\" for i in [1,2,3,4,5,12,13,14,15,17,18,19,20]]\n",
    "name_list[0] = \"train1_\"\n",
    "\n",
    "t_type = [\" \", \"no debias\", \"max\\n\"+r\"$\\alpha$ = 0.1\", \"max\\n\"+r\"$\\alpha$ = 0.05\", \n",
    "                      \"max\\n\"+r\"$\\alpha$ = 0.01\", \"max\\n\"+r\"$\\alpha$ = 0.001\",\n",
    "                      \"max 5\\n\"+r\"$\\alpha$ = 0.1\", \"max 5\\n\"+r\"$\\alpha$ = 0.05\", \n",
    "                      \"max 5\\n\"+r\"$\\alpha$ = 0.01\", \"max 5\\n\"+r\"$\\alpha$ = 0.001\",\n",
    "                      \"gaussian\\n\"+r\"$\\alpha$ = 0.1\", \"gaussian\\n\"+r\"$\\alpha$ = 0.05\", \n",
    "                      \"gaussian\\n\"+r\"$\\alpha$ = 0.01\", \"gaussian\\n\"+r\"$\\alpha$ = 0.001\"]\n",
    "y_lim = [0, 100]\n",
    "\n",
    "utils.make_box_plot(df_final_results, name_list, training_type=t_type, colors=colors, y_lim=y_lim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Msc_AI_FACT_Cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
